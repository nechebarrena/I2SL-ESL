{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "I2SL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/Ad5GWuyaE3992SUc7N9x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nechebarrena/I2SL-ESL/blob/main/I2SL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbbuxoUSgPvG"
      },
      "source": [
        "# Notas del libro An introduction to statistical learning\n",
        "\n",
        "Índice de temas para ver:\n",
        "\n",
        "1. Introducción ---\n",
        "(2.1) - What Is Statistical Learning? \n",
        "(2.2) - Assessing Model Accuracy\n",
        "\n",
        "2. Regresión lineal --- \n",
        "(3.1) - Simple Linear Regression\n",
        "(3.2) - Multiple Linear Regression\n",
        "\n",
        "3. Clasificación ---\n",
        "(4.1) - An Overview of Classification\n",
        "(4.2) - Why Not Linear Regression?\n",
        "(4.3) - Logistic Regression\n",
        "(4.4) - Linear Discriminant Analysis\n",
        "\n",
        "4. Cosas no lineales ---\n",
        "(7.1) - Polynomial Regression\n",
        "\n",
        "5. No supervisado --- \n",
        "(10.1) - The Challenge of Unsupervised Learning\n",
        "(10.2) - Principal Components Analysis\n",
        "(10.3) - Clustering Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIf5ylH9qnPN"
      },
      "source": [
        "## 1. Introducción\n",
        "### (2.1) - What Is Statistical Learning?\n",
        "\n",
        "Primer ejemplo de un algoritmo que \"aprende\" algo de los datos empíricos.\n",
        "Tenemos un conjunto de observaciones $Y$ que deseamos \"entender\" y otro conjunto de datos  $X_1,X_2,...,X_p$ que creemos explican el comportamiento de $Y$. Podemos suponer que existe un modelo tal que:\n",
        "\n",
        "$Y = f(X) + \\epsilon$\n",
        "\n",
        "donde $f$ es una función desconocida de un conjunto de variables $X_1,X_2,...,X_p$, y $\\epsilon$ es un error aleatorio independiente de $X$ y con media cero (si no fuese cero la podríamos incluir en el modelo).\n",
        "\n",
        "En general decimos que el aprendizaje estadístico es un conjunto de técnicas y herramientas para buscar la $f$ que mejor aproxima al problema y también para poder evaluar cuán buena es esta solución.\n",
        "\n",
        "#### ¿Por qué queremos estimar $f$?\n",
        "\n",
        "En general podemos pensar en 2 motivos distintos. Estos son predecir e inferir.\n",
        "Cuando usamos un modelo para predecir es porque tenemos un conjunto de mediciones sobre las variables $X_1,X_2,...,X_p$ y queremos conocer cuál es el valor $Y$ asociado. En tal caso podemos pensar que tenemos un modelo $\\hat f$ que aproxima la función desconocida $f$ y que al aplicarlo a los datos obtenemos una predicción $\\hat Y$,\n",
        "\n",
        "$\\hat Y = \\hat f(X)$\n",
        "\n",
        "La predicción que podamos hacer $\\hat Y$ trae consigo 2 errores. El primero que podemos considerar es el error *reducible*. Este error proviene de como ajustamos $\\hat f$ a los datos. Este error se dice reducible porque si mejoramos la técnica podemos mejorar el ajuste. Sin embargo existe un error *irreducible* que antes escribimos como $\\epsilon$. Este error no puede ser eliminado porque forma parte del proceso mismo. En cierta forma lo que decimos con esto es que $Y$ no solo depende de $X_1,X_2,...,X_p$ sino que también depende de $\\epsilon$ de una forma fundamental. La pregunta es, ¿por qué tenemos un error irreducible?... La respuesta más sencilla es porque no conocemos ni podemos medir todas las variables que realmente afectan al proceso. En definitiva es una manifestación del problema del demonio de Laplace. Si pudiésemos conocer y medir tooodas las variables que afectan a un determinado prceso, entonces no tendríamos, a priori, un error irreducible. Pero bueno, esto es obviamente imposible.\n",
        "\n",
        "Podemos escribir el valor de expectación del error cuadrático como:\n",
        "\n",
        "$E[(Y - \\hat Y)^2] = E[(f(x) + \\epsilon - \\hat f(x))^2] = (f(x) - \\hat f(x))^2 + Var(\\epsilon)$\n",
        "\n",
        "donde $(f(x) - \\hat f(x))^2$ es el error reducible y $Var(\\epsilon)$ es el error irreducible. Está claro que el error reducible viene de la diferencia entre el modelo ajustado $\\hat f$ y el modelo desconocido $f$, mientras que el error irreducible proviene del desconocimiento que tenemos del sistema, el cual encapsulamos en $\\epsilon$. En general las técnicas de aprendizaje estadístico buscan minimzar el error reducible.\n",
        "\n",
        "Por otro lado, también podemos buscar un modelo no para hacer predicciones sino para hacer inferencia. En esta situación lo que buscamos es poder entender el fenómeno $f$ y cómo este depende de los features $X_1,X_2,...,X_p$. En general buscamos averiguar cuál es la importancia relativa de cada feature en el resultado final $Y$ y cuáles son las formas funcionales (en el mejor de los casos) de estas dependencias.\n",
        "\n",
        "### ¿Cómo estimamos $f$?\n",
        "\n",
        "A lo largo del libro vamos a analizar distintos métodos, lineales y no lineales para aproximar $f$, sin embargo todos comparten ciertas carecterísticas. Por empezar; siempre vamos a considerar que disponemos de $n$ mediciones experimentales (u observaciones directamente) del fenómeno que queremos estudiar. En general podemos separar los métodos (sean lineales o no lineales) en dos categorías, los métodos paramétricos y los no paramétricos.\n",
        "\n",
        "#### Métodos paramétricos\n",
        "Los métodos paramétricos son aquellos que asumen inicialmente una forma funcional para la $f$ que vamos a ajustar y permiten ajustar los parámetros de dicha forma funcional para ajustar las predicciones a los datos experimentales. O sea, fijamos inicialmente una forma funcional con una determinada cantidad de parámetros (puede ser aribitrariamente grande) y tratamos de averiguar cuál es el conjunto de ellos que mejor permite ajustar los datos. Un ejemplo muy evidente de este tipo de métodos es un ajuste lineal.\n",
        "\n",
        "$f(X) = \\beta_0 + \\beta_1 x_1 + \\beta_1 x_1 $ \n",
        "$f(X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...+ \\beta_p x_p \\approx Y $\n",
        "\n",
        "En este caso la forma funcional de $f$ está prescripta en un modelo lineal, los parámetros que debemos encontrar para que el modelo ajuste lo mejor posible a los datos experimentales $Y$ son los coeficientes $(\\beta_0,\\beta_1,\\beta_2,...\\beta_p)$. \n",
        "\n",
        "#### Métodos NO paramétricos\n",
        "\n",
        "Contrariamente a los primeros, los métodos no paramétricos, no definen inicialmente la forma funcional de $f$. Más adelante, cuando veamos algún ejemplo de modelo no paramétrico, probablemente quede más claro cómo funcionan. Ahora, aunque el libro mucho no aclara, yo arriesgo a decir que obviamente estos modelos también tienen parámetros de ajuste, pero a diferencia de los primeros solemos llamarlos hiperparámetros, pues permiten ajustar al modelo pero no lo prescriben. Dicho de otra forma, prescriben el método pero no la forma de $f$. Vuelvo a recordar que estas son mis palabras y no las del libro.\n",
        "Lo que sí dice el libro es que estos métodos suelen ser más propensos al overfit. Esto tiene sentido pues en principio la forma funcional de $f$ puede terminar siendo todo lo complicada que uno permita. Por otro lado, la ventaja que tienen sobre los métodos paramétricos es que pueden ajustar a modelos más complejos de los que uno espera inicialmente.\n",
        "\n",
        "### El balance entre la precisión del modelo y su interpretabilidad \n",
        "\n",
        "A lo largo del libro vamos a analizar una amplia variedad de métodos, paramétricos y no paramétricos, cada uno con cualidades distintas. Estas cualidades terminan redundando en dos características distintas, que en general, se mueven en direcciones opuestas: la flexibilidad del modelo y su interpretabilidad. Un modelo flexible es aquél que permite que la función $f$ varíe más según los datos de entrenamiento. Sin embargo, esta flexibilidad del modelo para buscar $f$ termina redundando en una peor intepretabilidad del mismo. En el extremo opuesto, un modelo menos flexible suele ser más fácil de interpretar. Volvamos al caso del modelo lineal. Este modelo solo permite que $f$ se mueva en el conjunto de las funciones lineales independientemente de cuáles sean los datos de entrenamiento. Esto muestra lo poco flexible que es, sin embargo, su interpretación es bastante directa, porque ya sabemos que cada coeficiente $\\beta_i$ está asociado al comportamiento de la variable $x_i$. \n",
        "\n",
        "En general podemos decir que cuando nuestro objetivo es la inferencia los modelos menos flexibles pero más fáciles de interpretar van a sernos más útiles, mientras que cuando nuestro objetivo está puesto en la predicción podemos recaer en modelos menos interpretables pero más flexibles.\n",
        "\n",
        "### Aprendizaje supervisado VS aprendizaje no supervisado\n",
        "\n",
        "En general los problemas que se nos presenten y que abordemos utilizando técnicas estadísticas pueden venir en dos formas distintas. Por un lado están los problemas donde tenemos un conjunto de mediciones $X$ que podemos identificar como variables  o predictores, y un conjunto de mediciones $Y$ que asociamos a una variable objetivo o target. A estos problemas los atacamos con métodos que llamamos supervisados. Y decimos que son supervisados porque nosotros ajustamos un modelo donde le enseñamos a nuestra $f$ que dado un cierto $X$ responda con un cierto $Y$. Sin embargo existen otro tipo de problemas donde (al menos a priori) no podemos separar entre variables predictoras y variables objetivos. En este caso todas nuestras variables son variables predictoras. Esta situación presenta un problema porque no podemos enseñarle a ninguna función $f$ a que se parezca a ningún valor objetivo $Y$ simplemente porque no lo tenemos. En este caso nuestro objetivo cambia y pasamos a tratar de entender las características de nuestros datos en función de las variables que disponemos. En general intentamos separar nuestros datos según criterios que nos permitan identificar grupos con comportamientos identificables. A estos modelos se los suele llamar de \"clustering\" porque, en resumen, lo que buscan hacer es encontrar clusters de datos en la totalidad de ellos. \n",
        "\n",
        "También existen los problemas semi-supervisados, que son aquellos donde una parte de nuestros datos posee una variable objetivo y otra parte no. De hecho, este es un problema muy común. Sin embargo no lo vamos a abordar en este libro =(.\n",
        "\n",
        "### Clasificación VS regresión\n",
        "\n",
        "Otra posible división que podemos hacer entre nuestros metodos es aquella que separa el tipo de variable que tenemos como objetivo. En general decimos que si nuestra variable objetivo es un valor numérico (continuo) debemos realizar una regresión para poder predecir con nuestro modelo, mientras que si la variable objetivo es categórica, lo que debemos hacer es una clasificación para poder predecir en que categoría caerá una nueva instancia. Sin embargo esta división es muchas veces algo arbitraria y en general los métodos estadísticos sirven para, con leves modificaciones, adaptarse a problemas de clasificación o de regresión.\n",
        "\n",
        "### Evaluando la precisión del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAsiLUD4RWcf"
      },
      "source": [
        ""
      ]
    }
  ]
}