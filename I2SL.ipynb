{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "I2SL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPtLzwJOzB31L8eCcdcOQGl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nechebarrena/I2SL-ESL/blob/main/I2SL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbbuxoUSgPvG"
      },
      "source": [
        "# Notas del libro An introduction to statistical learning\n",
        "\n",
        "Indice de temas para ver:\n",
        "\n",
        "1. Introduccion ---\n",
        "(2.1) - What Is Statistical Learning? \n",
        "(2.2) - Assessing Model Accuracy\n",
        "\n",
        "2. Regresion lineal --- \n",
        "(3.1) - Simple Linear Regression\n",
        "(3.2) - Multiple Linear Regression\n",
        "\n",
        "3. Clasificacion ---\n",
        "(4.1) - An Overview of Classification\n",
        "(4.2) - Why Not Linear Regression?\n",
        "(4.3) - Logistic Regression\n",
        "(4.4) - Linear Discriminant Analysis\n",
        "\n",
        "4. Cosas no lineales ---\n",
        "(7.1) - Polynomial Regression\n",
        "\n",
        "5. No supervisado --- \n",
        "(10.1) - The Challenge of Unsupervised Learning\n",
        "(10.2) - Principal Components Analysis\n",
        "(10.3) - Clustering Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIf5ylH9qnPN"
      },
      "source": [
        "## 1. Introduccion\n",
        "### (2.1) - What Is Statistical Learning?\n",
        "\n",
        "Primer ejemplo de un algoritmo que \"aprende\" algo de los datos empiricos.\n",
        "Tenemos un conjunto de observaciones $Y$ que deseamos \"entender\" y otro conjunto de datos  $X_1,X_2,...,X_p$ que creemos explican el comportamiento de $Y$. Podemos suponer que existe un modelo tal que:\n",
        "\n",
        "$Y = f(X) + \\epsilon$\n",
        "\n",
        "Donde $f$ es una funcion desconocida de un conjunto de variables $X_1,X_2,...,X_p$, y $\\epsilon$ es un error aleatorio independiente de $X$ y con media cero (si no fuese cero la podriamos incluir en el modelo).\n",
        "\n",
        "En general decimos que el aprendizaje estadistico es un conjunto de tecnicas y herramientas para buscar la $f$ que mejor aproxima al problema y tambien para poder evaluar cuan buena es esta solucion.\n",
        "\n",
        "#### ¿Por que queremos estimar $f$?\n",
        "\n",
        "En general podemos pensar en 2 motivos distintos. Estos son, predecir e inferir.\n",
        "Cuando usamos un modelo para predecir es porque tenemos un conjunto de mediciones sobre las variables $X_1,X_2,...,X_p$ y queremos conocer cual es el valor $Y$ asociado. En tal caso podemos pensar que tenemos un modelo $\\hat f$ que aproxima la funcion desconocida $f$ y que al aplicarlo a los datos obtenemos una prediccion $\\hat Y$.\n",
        "\n",
        "$\\hat Y = \\hat f(X)$\n",
        "\n",
        "La prediccion que podamos hacer $\\hat Y$ trae consigo 2 errores. El primero que podemos considerar es el error *reducible*. Este error proviene de como ajustamos $\\hat f$ a los datos. Este error se dice reducible porque si mejoramos la tecnica podemos mejorar el ajuste. Sin embargo existe un error *irreducible* que antes escribimos como $\\epsilon$. Este error no puede ser eliminado porque forma parte del proceso mismo. En cierta forma lo que decimos con esto es que $Y$ no solo depende de $X_1,X_2,...,X_p$ sino que tambien depende de $\\epsilon$ de una forma fundamental. La pregunta es, ¿por que tenemos un error irreducible?...La respuesta mas sencilla es porque no conocemos ni podemos medir todas las variables que realmente afectan al proceso. En definitiva es una manifestacion del problema del demonio de Laplace. Si pudiesemos conocer y medir tooodas las variables que afectan a un determinado prceso, entonces no tendriamos, a priori, un error irreducible. Pero bueno, esto es obviamente imposible.\n",
        "\n",
        "Podemos escribir el el valor de expectacion del error cuadratico como:\n",
        "\n",
        "$E[(Y - \\hat Y)^2] = E[(f(x) + \\epsilon - \\hat f(x))^2] = (f(x) - \\hat f(x))^2 + Var(\\epsilon)$\n",
        "\n",
        "Donde $(f(x) - \\hat f(x))^2$ es el error reducible y $Var(\\epsilon)$ es el error irreducible. Esta claro que el error reducible viene de la diferencia entre el modelo ajustado $\\hat f$ y el modelo desconocido $f$, mientras que el error irreducible proviene del desconocimiento que tenemos del sistema, el cual encapsulamos en $\\epsilon$. En general las tecnicas de aprendizaje estadistico buscan minimzar el error reducible.\n",
        "\n",
        "Por otro lado, tambien podemos buscar un modelo no para hacer predicciones sino para hacer inferencia. En esta situacion lo que buscamos es poder entender el fenomeno $f$ y como este depende de los features $X_1,X_2,...,X_p$. En general buscamos averiguar cual es la importancia relativa de cada feature en el resultado final $Y$ y cuales son las formas funcionales (en el mejor de los casos) de estas dependencias."
      ]
    }
  ]
}