{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "I2SL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuwNLFs+KEqCWg0Dabd+0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nechebarrena/I2SL-ESL/blob/main/I2SL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbbuxoUSgPvG"
      },
      "source": [
        "# Notas del libro An introduction to statistical learning\n",
        "\n",
        "Indice de temas para ver:\n",
        "\n",
        "1. Introduccion ---\n",
        "(2.1) - What Is Statistical Learning? \n",
        "(2.2) - Assessing Model Accuracy\n",
        "\n",
        "2. Regresion lineal --- \n",
        "(3.1) - Simple Linear Regression\n",
        "(3.2) - Multiple Linear Regression\n",
        "\n",
        "3. Clasificacion ---\n",
        "(4.1) - An Overview of Classification\n",
        "(4.2) - Why Not Linear Regression?\n",
        "(4.3) - Logistic Regression\n",
        "(4.4) - Linear Discriminant Analysis\n",
        "\n",
        "4. Cosas no lineales ---\n",
        "(7.1) - Polynomial Regression\n",
        "\n",
        "5. No supervisado --- \n",
        "(10.1) - The Challenge of Unsupervised Learning\n",
        "(10.2) - Principal Components Analysis\n",
        "(10.3) - Clustering Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIf5ylH9qnPN"
      },
      "source": [
        "## 1. Introduccion\n",
        "### (2.1) - What Is Statistical Learning?\n",
        "\n",
        "Primer ejemplo de un algoritmo que \"aprende\" algo de los datos empiricos.\n",
        "Tenemos un conjunto de observaciones $Y$ que deseamos \"entender\" y otro conjunto de datos  $X_1,X_2,...,X_p$ que creemos explican el comportamiento de $Y$. Podemos suponer que existe un modelo tal que:\n",
        "\n",
        "$Y = f(X) + \\epsilon$\n",
        "\n",
        "Donde $f$ es una funcion desconocida de un conjunto de variables $X_1,X_2,...,X_p$, y $\\epsilon$ es un error aleatorio independiente de $X$ y con media cero (si no fuese cero la podriamos incluir en el modelo).\n",
        "\n",
        "En general decimos que el aprendizaje estadistico es un conjunto de tecnicas y herramientas para buscar la $f$ que mejor aproxima al problema y tambien para poder evaluar cuan buena es esta solucion.\n",
        "\n",
        "#### ¿Por que queremos estimar $f$?\n",
        "\n",
        "En general podemos pensar en 2 motivos distintos. Estos son, predecir e inferir.\n",
        "Cuando usamos un modelo para predecir es porque tenemos un conjunto de mediciones sobre las variables $X_1,X_2,...,X_p$ y queremos conocer cual es el valor $Y$ asociado. En tal caso podemos pensar que tenemos un modelo $\\hat f$ que aproxima la funcion desconocida $f$ y que al aplicarlo a los datos obtenemos una prediccion $\\hat Y$.\n",
        "\n",
        "$\\hat Y = \\hat f(X)$\n",
        "\n",
        "La prediccion que podamos hacer $\\hat Y$ trae consigo 2 errores. El primero que podemos considerar es el error *reducible*. Este error proviene de como ajustamos $\\hat f$ a los datos. Este error se dice reducible porque si mejoramos la tecnica podemos mejorar el ajuste. Sin embargo existe un error *irreducible* que antes escribimos como $\\epsilon$. Este error no puede ser eliminado porque forma parte del proceso mismo. En cierta forma lo que decimos con esto es que $Y$ no solo depende de $X_1,X_2,...,X_p$ sino que tambien depende de $\\epsilon$ de una forma fundamental. La pregunta es, ¿por que tenemos un error irreducible?...La respuesta mas sencilla es porque no conocemos ni podemos medir todas las variables que realmente afectan al proceso. En definitiva es una manifestacion del problema del demonio de Laplace. Si pudiesemos conocer y medir tooodas las variables que afectan a un determinado prceso, entonces no tendriamos, a priori, un error irreducible. Pero bueno, esto es obviamente imposible.\n",
        "\n",
        "Podemos escribir el el valor de expectacion del error cuadratico como:\n",
        "\n",
        "$E[(Y - \\hat Y)^2] = E[(f(x) + \\epsilon - \\hat f(x))^2] = (f(x) - \\hat f(x))^2 + Var(\\epsilon)$\n",
        "\n",
        "Donde $(f(x) - \\hat f(x))^2$ es el error reducible y $Var(\\epsilon)$ es el error irreducible. Esta claro que el error reducible viene de la diferencia entre el modelo ajustado $\\hat f$ y el modelo desconocido $f$, mientras que el error irreducible proviene del desconocimiento que tenemos del sistema, el cual encapsulamos en $\\epsilon$. En general las tecnicas de aprendizaje estadistico buscan minimzar el error reducible.\n",
        "\n",
        "Por otro lado, tambien podemos buscar un modelo no para hacer predicciones sino para hacer inferencia. En esta situacion lo que buscamos es poder entender el fenomeno $f$ y como este depende de los features $X_1,X_2,...,X_p$. En general buscamos averiguar cual es la importancia relativa de cada feature en el resultado final $Y$ y cuales son las formas funcionales (en el mejor de los casos) de estas dependencias.\n",
        "\n",
        "### ¿Como estimamos $f$?\n",
        "\n",
        "A lo largo del libro vamos a analizar distintos metodos, lineales y no lineales para aproximar $f$, sin embargo todos comparten ciertas carecteristicas. Por empezar; Siempre vamos a considerar que disponemos de $n$ mediciones experimentales (u observaciones directamente) del fenomeno que queremos estudiar. En general podemos separar los metodos (sean lineales o no lineales) en dos categorias, los metodos parametricos y los no parametricos.\n",
        "\n",
        "#### Metodos parametricos\n",
        "Los metodos parametricos son aquellos que asumen inicialmente una forma funcional para la $f$ que vamos a ajustar y permiten ajustar los parametros de dicha forma funcional para ajustar las predicciones a los datos experimentales. O sea, fijamos inicialmente una forma funcional con una determinada cantidad de parametros (puede ser aribitrariamente grande) y tratamos de averiguar cual es el conjunto de ellos que mejor permite ajustar los datos. Un ejemplo muy evidente de este tipo de metodos es un ajuste lineal.\n",
        "\n",
        "$f(X) = \\beta_0 + \\beta_1 x_1 + \\beta_1 x_1 $ \n",
        "$f(X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...+ \\beta_p x_p \\approx Y $\n",
        "\n",
        "En este caso la forma funcional de $f$ esta prescripta en un modelo lineal, los parametros que debemos encontrar para que el modelo ajuste lo mejor posible a los datos experimentales $Y$ son los coeficientes $(\\beta_0,\\beta_1,\\beta_2,...\\beta_p)$. \n",
        "\n",
        "#### Metodos NO parametricos\n",
        "\n",
        "Contrariamente a los primeros, los metodos no parametricos, no definen inicialmente la forma funciona de $f$. Mas adelante, cuando veamos algun ejemplo de modelo no parametrico, probablemente quede mas claro como funcionan. Ahora, aunque el libro mucho no aclara, yo arriesgo a decir, que obviamente estos modelos tambien tienen parametros de ajuste, pero a diferencia de los primeros solemos llamarlos hiperparametros, pues permiten ajustar al modelo pero no lo prescriben. Dicho de otra forma, prescriben el metodo pero no la forma de $f$. Vuelvo a recordar que estas son mis palabras y no las del libro.\n",
        "Lo que si dice el libro es que estos metodos suelen ser mas propensos al overfit. Esto tiene sentido pues en principio la forma funcional de $f$ puede terminar siendo todo lo complicada que uno permita. Por otro lado, la ventaja que tienen sobre los metodos parametricos es que pueden ajustar a modelos mas complejos de los que uno espera inicialmente.\n",
        "\n",
        "### El balance entre la precision del modelo y su interpretatibilidad \n",
        "\n",
        "A lo largo del libro vamos a analizar una amplia variedad de metodos, parametricos y no parametricos, cada uno con cualidades distintas. Estas cualidades terminan redundando en dos caracteristicas distintas, que en general, se mueven en direcciones opuestas, la flexibilidad del modelo y su interpretatibilidad. Un modelo flexible es aquel que permite que la funcion $f$ varie mas segun los datos de entrenamiento. Sin embargo, esta flexibilidad del modelo para buscar $f$ termina redundando en una peor intepretatibilidad del mismo. En el extremo opuesto, un modelo menos flexible suele ser mas facil de interpretar. Volvamos al caso del modelo lineal. Este modelo solo permite que $f$ se mueva en el conjunto de las funciones lineales independientemente de cuales sean los datos de entrenamiento. Esto muestra lo poco flexible que es, sin embargo, su interpretacion es bastante directa, porque ya sabemos que cada coeficiente $\\beta_i$ esta asociado al comportamiento de la variable $x_i$. \n",
        "\n",
        "En general podemos decir que cuando nuestro objetivo es la inferencia los modelos menos flexibles pero mas faciles de interpretar van a sernos mas utiles, mientras que cuando nuestro objetivo esta puesto en la prediccion podemos recaer en modelos menos interpretables pero mas flexibles.\n",
        "\n",
        "### Aprendizaje supervisado VS aprendizaje no supervisado\n",
        "\n",
        "En general los problemas que se nos presenten y que abordemos utilizando tecnicas estadisticas pueden venir en dos formas distintas. Por un lado estan los problemas donde tenemos un conjunto de mediciones $X$ que podemos identificar como variables  o predictores, y un conjunto de mediciones $Y$ que asociamos a una variable objetivo o target. A estos problemas los atacamos con metodos que llamamos supervisados. Y decimos que son supervisados porque nosotros ajustamos un modelo donde le enseñamos a nuestra $f$ que dado un cierto $X$ responda con un cierto $Y$. Sin embargo existen otro tipo de problemas donde (al menos a priori) no podemos separar entre variables predictoras y variables objetivos. En este caso todas nuestras variables son variables predictoras. Esta situacion presenta un problema porque no podemos enseñarle a ninguna funcion $f$ a que se parezca a ningun valor objetivo $Y$ simplemente porque no lo tenemos. En este caso nuestro objetivo cambia y pasamos a tratar de entender las caracteristicas de nuestros datos en funcion de las variables que disponemos. En general intentamos separar nuestros datos segun criterios que nos permitan identificar grupos con comportamientos identificables. A estos modelos se los suele llamar de \"clustering\" porque, en resumen, lo que buscan hacer es encontrar clusters de datos en la totalidad de ellos. \n",
        "\n",
        "Tambien existen los problemas semi-supervisados, que son aquellos donde una parte de nuestros datos posee una variable objetivo y otra parte no. De hecho, este es un problema muy comun. Sin embargo no lo vamos a abordar en este libro =(.\n"
      ]
    }
  ]
}